---
title: "Quarto Dia - Rodando Alguns Modelos"
author: "Pedro Cavalcante"
date: "20 de fevereiro de 2019"
output: html_document
---

Agora que cobrimos manipulação, importação, limpeza e exploração de dados, podemos finalmente analisa-los. Hoje vamos cobrir algumas rotinas estatísticas, como roda-las e executar alguns testes de diagnósticos. Vamos usar algumas bases de dados do pacote ``wooldridge``, então o tenha instalado.


# Um primeiro exemplo de regressão linear

Começaremos com um exemplo de dados simulados, em que conhecemos todos os parâmetros envolvidos. Note que a função do R padrão para rodar modelos lineares pelo método dos mínimos quadrados ordinários é ``lm()``. Sua saída é uma lista e podemos recuperar algumas informções dela como resíduos usando a função ``resid()`` e seus coeficientes com a função ``coef()``. 

Modelos no R usam _fórmulas_. Uma fórmula de R *sempre* está na forma ``variável explicada ~ variável explicativa 1 + variável explicativa 2 + ...``. As variáveis podem ser manipuladas com funções como ``exp()``, ``log()`` e ``sqrt()``. 


```{R}
set.seed(1234)

n = 5000 # tamanho da amostra

X = runif(n = n, min = 10, max = 20) # n = 2000 de uma variável aleatória X ~ U(10,20)
u = rnorm(n = n) # pertubações aleatórias com distribuição u ~ N(0,1)
Y = 5 + 0.8*X + u   # criamos Y a partir de X e u

dados = data.frame(explicada = Y,
                   explicativa = X)

modelo1 = lm(Y ~ X, # fórmula do modelo a ser estimado 
             data = dados) # data.frame em que estão as variáveis

summary(modelo1) # visualizamos a tabela de regressão

modelo2 = lm(log(Y) ~ log(X), # fórmula do modelo a ser estimado 
             data = dados) # data.frame em que estão as variáveis

summary(modelo2) # modelo estimando elasticidades


```

Conhecemos os parâmetros verdadeiros da variável Y:

$$Y = 5 + 0.8X + u$$
O modelo que estimamos é extremamente compatível com isso. Os resíduos têm distribuição razoavelmente simétrica - mais à frente veremos se é, mais ainda, normal - e os parâmetros estimados estão muito próximos dos verdadeiros. Não só isso, como estatisticamente significantes. 

# Normalidade dos resíduos

O Teorema de Gauss-Markov garante que o estimador de Mínimos Quadrados Ordinários é o mais eficiente se, entre outras condições, os resíduos têm distribuição normal. Pois, vamos averiguar a normalidade dos resíduos do nosso modelo.

```{R}
residuos1 = resid(modelo1) # extraímos os resíduos do primeiro modelo
var(residuos1)
summary(residuos1) # sumário da distribuição
shapiro.test(residuos1) # realizamos teste de Shapiro-Wilk para normalidade
```

Podemos inspecionar graficamente também:

```{R, warning = FALSE, message = FALSE, dpi = 250}

library(dplyr)
library(ggplot2)

dados_residuos1 = data.frame(residuos = residuos1)

dados_residuos1  %>%
  ggplot(aes(x = residuos)) + 
  geom_histogram(aes(y = ..density..)) +  # y = ..density.. para termos densidade
  stat_function(fun = "dnorm", # normal
                args = list(sd = sqrt(var(residuos1))), # desvio-padrão igual ao dos resíduos
                size = 2) 


```


# Consistência assintótica do estimador de Mínimos Quadrados Ordinários

Espera-se que a "precisão" do estimador aumente à medida que o tamanho da amostra aumenta, não? De maneira mais formal, um estimador é dito consistente se à medida que o tamanho da amostra aumenta a variância do estimador diminui. De fato, podemos visualizar uma parte isso ocorrendo. Vamos usar um loop ``for`` para achar parâmetros estimados para tamanhos diferentes de amostras

```{R, warning = FALSE, message = FALSE, dpi = 250}

parametros = vector() # vetor vazio que será preenchido
variancia_estimador = vector()
m = 10000 # tamanho máximos de amostra

for(i in 1:m) {
  
X = runif(n = m, min = 10, max = 20) # n = 2000 de uma variável aleatória X ~ U(10,20)
u = rnorm(n = m) # pertubações aleatórias com distribuição u ~ N(0,1)
Y = 5 + 0.8*X + u   # criamos Y a partir de X e u

dados = data.frame(explicada = Y,
                   explicativa = X)

modelo = lm(Y ~ X, # fórmula do modelo a ser estimado 
             data = dados) # data.frame em que estão as variáveis
  
parametros[i] = coef(modelo)[2] # pegamos somente o parâmetro estimado para X
variancia_estimador[i] = var(parametros)

}

df_parametros = data.frame(Estimado = parametros,
                           Amostra = 1:m) #criamos um dataframe para o ggplot2
df_variancia = data.frame(Variancia = variancia_estimador,
                          amostra = 1:m)

df_variancia %>%
  ggplot(aes(x = amostra, y = Variancia)) +
  geom_line(size = 2)

df_parametros %>%
  ggplot(aes(x = Estimado)) +
  geom_histogram(aes(y = ..density..)) + 
  geom_vline(xintercept = .8, # linha vertical no parâmetro verdadeiro
             size = 2) +
  ylab("") +
  xlab("Parâmetro estimado") +
  labs(title = "Normalidade do estimador mínimos quadrados")

df_parametros %>%
  ggplot(aes(x = Amostra, y = Estimado)) +
  geom_line() + 
  ylab("Parâmetro Estimado") + 
  labs(title = "Consistência Assintótica do Estimador de Mínimos Quadrados",
       subtitle = "Os parâmetros estimados se comportam como ruído branco, com média no parâmetro verdadeiro")


```

# Dados reais, Testes In-Sample/Out-of-Sample e alguns diagnósticos

Validar um modelo é importante, nem só de p-valor e R^2 vive o homem. Para isso, normalmente fazemos testes in-sampla/out-of-sample. A ideia é simples: estimamos um modelo em uma amostra dos dados e realizamos previsões em outra amostra para testar o quão preciso é o modelo. Vamos por partes.

Primeiro vamos importar dados, separar observações incompletas e depois divir a base em uma de treino e outra de teste. Existem várias maneiras de fazer isso, a mais simples é usar a função ``sample()``. O problema desse método é que não garante uma divisão correta da base de dados pois uma mesma observação pode ser amostrada duas vezes seguidas. Um método mais interessante é gerar uma variável nova na base com uma distribuição conhecida - eu sempre uso Uniforme - e selecionar observações em que essa variável tenha um valor acima ou abaixo de um nível. Por exemplo, se essa variável tiver distribuição Uniforme entre 0 e 1, baste selecionar todas as observações com valores menores que 0.75 para que você separe aproximadamente 75% dos dados para treino e 25% para teste. 

Uma nota de aviso sobre normalidade dos resíduos é que distribuições verdadeiramente normais não tem caudas. Em particular, o teste de Shapiro-Wilk irá prontamente rejeitar normalidade se houverem caudas nos dados, então é sempre de bom tom exlporar visualmente os resíduos e não confiar cegamente na estatística do teste. 

```{R}
library(wooldridge)
data("wage2")

head(wage2)

wage2$completo = complete.cases(wage2) # vetor dizendo se a observação é completa
wage2 = wage2[wage2$completo == TRUE,]
wage2$separador = runif(n = nrow(wage2), min = 0, max = 1)

treino = wage2[wage2$separador < .75,]
teste = wage2[wage2$separador >= .75,]

```

Agora que temos dados separados podemos estimar um modelo na base de treino:


```{R}
modelo_salarios = lm(lwage ~ hours + IQ + educ + exper + tenure + south + urban + black, 
                     data = treino)


summary(modelo_salarios)


###### Analisando os resíduos
residuos_salarios = resid(modelo_salarios)

resid(modelo_salarios) %>% shapiro.test()
```

O teste de Shapiro-Wilk não nos dá indícios de que não há normalidade nos resíduos, o que é bom.

```{R, warning = FALSE, message = FALSE}
dados_residuos_salarios = data.frame(residuos = residuos_salarios)

dados_residuos_salarios %>%
  ggplot(aes(x = residuos)) + 
  geom_histogram(aes(y = ..density..)) +  # y = ..density.. para termos densidade
  stat_function(fun = "dnorm", # normal
                args = list(sd = sqrt(var(residuos_salarios))), # desvio-padrão igual ao dos resíduos
                size = 2) 
```

Vamos agora avaliar a performance do nosso modelo. Para isso usaremos a função ``predict.lm()``. Ela recebe um objeto em que armazenamos um modelo e dados de teste.

```{R, warning = FALSE, message = FALSE}

teste$previsao = predict.lm(modelo_salarios, teste)
teste$erro = teste$lwage - teste$previsao 

summary(teste$erro)

teste %>%
  ggplot(aes(x = erro)) + 
  geom_histogram(aes(y = ..density..))

```

Esse exercício tem interpretação mais clara quando estamos realizando exercícios econométricos de classificação, mas é bom saber que nosso modelo estimado na média tem erro próximo de zero.


# Regressão Descontínua

É plausível que dia de nascimento seja relevante para renda e escolaridade? Muito pouco a princípio, a menos que - por exemplo - um pai precise esperar um ano para matricular seu filho numa escola pública porque ele nasceu um dia depois da data limite para o ano. Essa é a ideia de [McCrary e Royer (2011, AER)](https://www.nber.org/papers/w12329). Esse tipo de evento não causa só variação plausivelmente exógena na escolaridade entre crianças, mas variação aguda. Uma descontinuidade, por assim dizer. O gráfico abaixo, tirado do paper, ilustra isso:

![](https://i.imgur.com/SoWHVFw.png)

Esse desenho de pesquisa para estimar a diferença dos limites laterais no polinômio estimado é chamado de _Regression Discontinuity Design_, ou RDD. É uma técnica muito interessante porque normalmente estima o efeito _causal_ de uma variável sobre outra e não somente uma correlação. 

No site http://masteringmetrics.com/wp-content/uploads/2015/01/AEJfigs.dta temos dados de taxas de mortalidade por várias causas por grupos de idade. A variável ``age.cell`` diz quantos anos e meses de vida o grupo tinha e a variável ``mva`` tem a taxa de mortes para acidentes com veículos. Como nos EUA 21 é a idade para beber legalmente, espera-se algum tipo de descontinuidade nas mortes a partir dessa idade. Ela existe? Vamos averiguar isso com a implementação de RDD no pacote ``rdrobust``. Em particular, com a função ``rdplot()``.


```{R}
library(rdrobust)
library(haven)
mortes = read_dta("http://masteringmetrics.com/wp-content/uploads/2015/01/AEJfigs.dta")

mortes$idade = mortes$agecell - 21 # diferença do grupo de idade ao nível do tratamento

rdplot(y = mortes$mva,
       x = mortes$idade,
       x.label = "Diferença de idade para os 21 anos",
       y.label = "Mortes por acidentes de veículos a cada 100 mil habitantes",
       title = "Efeito de tratamento da idade legal para beber")
```

# Sobre o que nós *não* falamos aqui

Não tocamos em diagnósticos de heterocedasticidade ou autocorrelação, muito menos em alguns modelos para séries temporais. Também não tocamos em modelos não-lineares com a função ``nlm()``. Existe outra infinidade de assuntos que são interessantes. 

A ideia é que o leitor, conhecendo a sintaxe e tendo alguns exemplos e modelos sendo estimados, consiga sozinho e com a ajuda do Google, aprender a rodar modelos novos e mais interessantes. No fundo é essa a mentalidade necessária para lidar com dados e programar.


