---
title: "Quarto Dia - Rodando Alguns Modelos"
author: "Pedro Cavalcante"
date: "20 de fevereiro de 2019"
output: html_document
---

Agora que cobrimos manipulação, importação, limpeza e exploração de dados, podemos finalmente analisa-los. Hoje vamos cobrir algumas rotinas estatísticas, como roda-las e executar alguns testes de diagnósticos. Vamos usar algumas bases de dados do pacote ``wooldridge``, então o tenha instalado.


# Um primeiro exemplo de regressão linear

Começaremos com um exemplo de dados simulados, em que conhecemos todos os parâmetros envolvidos. Note que a função do R padrão para rodar modelos lineares pelo método dos mínimos quadrados ordinários é ``lm()``. Sua saída é uma lista e podemos recuperar algumas informções dela como resíduos usando a função ``resid()`` e seus coeficientes com a função ``coef()``. 

Modelos no R usam _fórmulas_. Uma fórmula de R *sempre* está na forma ``variável explicada ~ variável explicativa 1 + variável explicativa 2 + ...``. As variáveis podem ser manipuladas com funções como ``exp()``, ``log()`` e ``sqrt()``. 


```{R}
set.seed(1234)

n = 5000 # tamanho da amostra

X = runif(n = n, min = 10, max = 20) # n = 2000 de uma variável aleatória X ~ U(10,20)
u = rnorm(n = n) # pertubações aleatórias com distribuição u ~ N(0,1)
Y = 5 + 0.8*X + u   # criamos Y a partir de X e u

dados = data.frame(explicada = Y,
                   explicativa = X)

modelo1 = lm(Y ~ X, # fórmula do modelo a ser estimado 
             data = dados) # data.frame em que estão as variáveis

summary(modelo1) # visualizamos a tabela de regressão

modelo2 = lm(log(Y) ~ log(X), # fórmula do modelo a ser estimado 
             data = dados) # data.frame em que estão as variáveis

summary(modelo2) # modelo estimando elasticidades


```

Conhecemos os parâmetros verdadeiros da variável Y:

$$Y = 5 + 0.8X + u$$
O modelo que estimamos é extremamente compatível com isso. Os resíduos têm distribuição razoavelmente simétrica - mais à frente veremos se é, mais ainda, normal - e os parâmetros estimados estão muito próximos dos verdadeiros. Não só isso, como estatisticamente significantes. 

# Normalidade dos resíduos

O Teorema de Gauss-Markov garante que o estimador de Mínimos Quadrados Ordinários é o mais eficiente se, entre outras condições, os resíduos têm distribuição normal. Pois, vamos averiguar a normalidade dos resíduos do nosso modelo.

```{R}
residuos1 = resid(modelo1) # extraímos os resíduos do primeiro modelo
var(residuos1)
summary(residuos1) # sumário da distribuição
shapiro.test(residuos1) # realizamos teste de Shapiro-Wilk para normalidade
```

Podemos inspecionar graficamente também:

```{R, warning = FALSE, message = FALSE, dpi = 250}

library(dplyr)
library(ggplot2)

dados_residuos1 = data.frame(residuos = residuos1)

dados_residuos1  %>%
  ggplot(aes(x = residuos)) + 
  geom_histogram(aes(y = ..density..)) +  # y = ..density.. para termos densidade
  stat_function(fun = "dnorm", # normal
                args = list(sd = sqrt(var(residuos1))), # desvio-padrão igual ao dos resíduos
                size = 2) 


```


# Consistência assintótica do estimador de Mínimos Quadrados Ordinários

Espera-se que a "precisão" do estimador aumente à medida que o tamanho da amostra aumenta, não? De maneira mais formal, um estimador é dito consistente se à medida que a amostra aumenta a variância do estimador diminui. De fato, podemos visualizar uma parte isso ocorrendo. Vamos usar um loop ``for`` para achar parâmetros estimados para tamanhos diferentes de amostras

```{R, warning = FALSE, message = FALSE, dpi = 250}

parametros = vector() # vetor vazio que será preenchido
variancia_estimador = vector()
m = 10000 # tamanho máximos de amostra

for(i in 1:m) {
  
X = runif(n = m, min = 10, max = 20) # n = 2000 de uma variável aleatória X ~ U(10,20)
u = rnorm(n = m) # pertubações aleatórias com distribuição u ~ N(0,1)
Y = 5 + 0.8*X + u   # criamos Y a partir de X e u

dados = data.frame(explicada = Y,
                   explicativa = X)

modelo = lm(Y ~ X, # fórmula do modelo a ser estimado 
             data = dados) # data.frame em que estão as variáveis
  
parametros[i] = coef(modelo)[2] # pegamos somente o parâmetro estimado para X
variancia_estimador[i] = var(parametros)

}

df_parametros = data.frame(Estimado = parametros,
                           Amostra = 1:m) #criamos um dataframe para o ggplot2
df_variancia = data.frame(Variancia = variancia_estimador,
                          amostra = 1:m)

df_variancia %>%
  ggplot(aes(x = amostra, y = Variancia)) +
  geom_line(size = 2)

df_parametros %>%
  ggplot(aes(x = Estimado)) +
  geom_histogram(aes(y = ..density..)) + 
  geom_vline(xintercept = .8, # linha vertical no parâmetro verdadeiro
             size = 2) +
  ylab("") +
  xlab("Parâmetro estimado") +
  labs(title = "Normalidade do estimador mínimos quadrados")



df_parametros %>%
  ggplot(aes(x = Amostra, y = Estimado)) +
  geom_line() + 
  ylab("Parâmetro Estimado") + 
  labs(title = "Consistência Assintótica do Estimador de Mínimos Quadrados",
       subtitle = "Os parâmetros estimados se comportam como ruído branco, com média no parâmetro verdadeiro")


```


